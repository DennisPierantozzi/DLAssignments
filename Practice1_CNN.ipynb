{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b5996d8-1c7f-437d-ae75-deae468c7602",
   "metadata": {},
   "source": [
    "# Dennis Pierantozzi & Francesco Maria Mosca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eacc32a-52f1-49f9-ae0c-2c87c3093bac",
   "metadata": {},
   "source": [
    "# Practice 1 CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcda3cc-514f-48ac-be37-6c3f3429918c",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68c343d-7877-443c-bd33-fe5d23eeaa03",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ff91d61-dc63-434a-a67f-8ad7bbc49d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Define the relative paths to your train and test directories\n",
    "train_dir = os.path.join(current_dir, 'animals', 'train')\n",
    "test_dir = os.path.join(current_dir, 'animals', 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d79283-9f24-408e-9813-b852772ea506",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e1936f-6c5a-40cb-90bf-b50f8ae7d3d6",
   "metadata": {},
   "source": [
    "What have we done here? <br>\n",
    "We have used the function the professor suggested and preprocessed the data <br>\n",
    "for the train data the labels are inferred by the directory and categorical encoded. Size fixed in 120 and 120 and shuffled <br>\n",
    "for the test same but not shuffled since it is not needed <br>\n",
    "#### Created a validation dataset from the training dataset (split: 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89d8e656-7e81-4c7b-8923-5c44dd3616a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13474 files belonging to 5 classes.\n",
      "Using 10780 files for training.\n",
      "Found 13474 files belonging to 5 classes.\n",
      "Using 2694 files for validation.\n",
      "Found 1497 files belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import image_dataset_from_directory\n",
    "\n",
    "# Create TensorFlow datasets for training and validation\n",
    "train_data = image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',  # For one-hot encoded labels\n",
    "    image_size=(120, 120),  # Resize images to (120, 120)\n",
    "    batch_size=32,  # Batch size\n",
    "    shuffle=True,  # Shuffle the data\n",
    "    seed=123,  # Random seed for shuffling\n",
    "    validation_split=0.2,  # Fraction of training data to use for validation\n",
    "    subset='training'  # Use the training subset\n",
    ")\n",
    "\n",
    "validation_data = image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',  # For one-hot encoded labels\n",
    "    image_size=(120, 120),  # Resize images to (120, 120)\n",
    "    batch_size=32,  # Batch size\n",
    "    shuffle=False,  # Do not shuffle the data\n",
    "    seed=123,  # Random seed for shuffling\n",
    "    validation_split=0.2,  # Fraction of training data to use for validation\n",
    "    subset='validation'  # Use the validation subset\n",
    ")\n",
    "\n",
    "test_data = image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',  # For one-hot encoded labels\n",
    "    image_size=(120, 120),  # Resize images to (120, 120)\n",
    "    batch_size=32  # Batch size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632c04ad-7772-4a30-8098-a0b8cc805c18",
   "metadata": {},
   "source": [
    "### Data agumentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30268117-5469-4367-949e-4a945c28dd59",
   "metadata": {},
   "source": [
    "Why data agumentation? Which kind of things are we doing here? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2af62107-f2d3-40a1-8151-f300dbeaed45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.2),\n",
    "        layers.Rescaling(1./255)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71db105-c2ff-40eb-a5a3-181c7361a1be",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12900c66-905e-4e58-bfc5-b472d27d5b28",
   "metadata": {},
   "source": [
    "### Custom convolutional model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b856d956-1403-46d9-985e-750fe236e7f9",
   "metadata": {},
   "source": [
    "Our first idea has been to add 3 convulational layers with increasing filters. Kernel size 3 as in the lecture examples without padding or strides. <br> Then two pooling layers to reduce feature complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c55c89be-bb23-4188-91b2-655eb1ac6a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = keras.Input(shape=(120, 120, 3))\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(5, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab00c7d4-0b60-4daf-9c76-1eb729dc637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8154610-4b1a-46cd-80b1-04b1e635a3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 500ms/step - accuracy: 0.3781 - loss: 16.1459 - val_accuracy: 0.4488 - val_loss: 1.3307\n",
      "Epoch 2/10\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 411ms/step - accuracy: 0.7053 - loss: 0.8447 - val_accuracy: 0.7903 - val_loss: 0.7328\n",
      "Epoch 3/10\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 379ms/step - accuracy: 0.7593 - loss: 0.6590 - val_accuracy: 0.6756 - val_loss: 0.9648\n",
      "Epoch 4/10\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 399ms/step - accuracy: 0.8121 - loss: 0.5151 - val_accuracy: 0.9094 - val_loss: 0.3597\n",
      "Epoch 5/10\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 367ms/step - accuracy: 0.8435 - loss: 0.4440 - val_accuracy: 0.6255 - val_loss: 1.1357\n",
      "Epoch 6/10\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 360ms/step - accuracy: 0.8504 - loss: 0.4360 - val_accuracy: 0.9395 - val_loss: 0.2570\n",
      "Epoch 7/10\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 366ms/step - accuracy: 0.8821 - loss: 0.3563 - val_accuracy: 0.7910 - val_loss: 0.7823\n",
      "Epoch 8/10\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 364ms/step - accuracy: 0.8873 - loss: 0.3296 - val_accuracy: 0.8344 - val_loss: 0.7302\n",
      "Epoch 9/10\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 383ms/step - accuracy: 0.9031 - loss: 0.3035 - val_accuracy: 0.7702 - val_loss: 0.8844\n",
      "Epoch 10/10\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 395ms/step - accuracy: 0.9128 - loss: 0.2927 - val_accuracy: 0.6166 - val_loss: 2.1190\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=validation_data,\n",
    "    epochs=10  # Adjust the number of epochs as needed\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
